{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs: Exploring Guardian API Data\n",
    "Guardian news data provides us a range of different types of variable that we can use to get an overall picture of our datatset, and perhaps even find a some interesting patterns along the way.\n",
    "\n",
    "Below we look at a range of different options for examining Guardian Data. Whilst the text of the stories is obviously valuable data, we'll need more advanced text analysis methods for that. These methods allow us to get a good overall picture of our data and find general trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_json('AI_articles.json')\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the Data\n",
    "First we get the data ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the date column\n",
    "First we need to ensure that our data is clean and that the `webPublicationDate` is properly formatted as a `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['webPublicationDate'] = pd.to_datetime(articles['webPublicationDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking the Fields column\n",
    "The content of the `fields` column is determined when we collect our API data, by what we passed to `show-fields` in our query parameters. However what is returned is a dictionary of information. Ideally we want to expand these dictionaries out and create additional columns for each field (`byline`, `body` and `wordcount`).\n",
    "\n",
    "We'll mainly be using `wordcount` but the process will unpack all fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.loc[0, 'fields']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.json_normalize` happens to do this kind of job for us, but it will create an entirely new dataframe from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_field_data = pd.json_normalize(articles['fields'])\n",
    "articles_field_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having produced our dataframe of field data we just need to merge the `articles` dataframe and the new one together, matching up the indexes.\n",
    "When merging dataframes, left literally refers to the dataframe on the left of the operation, and right to the one most towards the right.\n",
    "\n",
    "`left.merge(right)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(articles_field_data, left_index=True, right_index=True)\n",
    "articles.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting wordcount to numeric\n",
    "Wordcount has been stored as a string. We can rectify that by using `.to_numeric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['wordcount'] = pd.to_numeric(articles['wordcount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Counts over time\n",
    "A key question of a dataset about the news, would when this news took place. Equally, we may also be interested in trends over time. Depending on your query, it may be interesting to see if there were changing publication rates related to your topic of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple `.describe` on the date column will tell us a little about the spread of the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['webPublicationDate'].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see trends, we can group our rows by publication period such as by `D`ay, `M`onth or `Y`ear. To do this we make a special time grouping object, and then group our data using it. We count the number of articles in each group and then plot them as a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouper = pd.Grouper(key='webPublicationDate', freq='M')\n",
    "count_over_time = articles[['webPublicationDate','id']].groupby(time_grouper).count().reset_index()\n",
    "count_over_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series are a little trickier to plot and Seaborn doesn't have a built in covnenience method for it. However we can use the `sns.lineplot` method to manually create one, and make some adjustments to size and label positioning manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plot = sns.lineplot(data=count_over_time, x='webPublicationDate', y='id')\n",
    "\n",
    "plot.tick_params(axis='x', labelrotation=45)\n",
    "plot.set(title='AI story Frequency over time', xlabel='Month', ylabel='Freq')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Filtering by a Date Range\n",
    "Using our timeseries plot we might decide to filter our data so we only work with a specific range period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_filter = articles['webPublicationDate'] >= 'January 2022'\n",
    "articles = articles[date_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['webPublicationDate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouper = pd.Grouper(key='webPublicationDate', freq='M')\n",
    "count_over_time = articles[['webPublicationDate','id']].groupby(time_grouper).count().reset_index()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plot = sns.lineplot(data=count_over_time, x='webPublicationDate', y='id')\n",
    "plot.tick_params(axis='x', labelrotation=45)\n",
    "plot.set(title='AI story Frequency over time January 2022+', xlabel='Month', ylabel='Freq')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appropriate Pillars?\n",
    "The Guardian has a number of major sections they refer to as Pillars. We can examine the distribution of our articles across these major categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pillar_counts = articles['pillarName'].value_counts()\n",
    "pillar_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=articles, y='pillarName', kind='count', order=pillar_counts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Filtering by Pillar\n",
    "Depending on your search query and the type of question you have, it may be worth filtering out material in unsuitable pillars, or focusing on just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_pillars = ['News', 'Opinion']\n",
    "pillar_filter = articles['pillarName'].isin(chosen_pillars)\n",
    "articles = articles[pillar_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering we can re-run our counts to check the filtering was applied, and produce a new visualisation of we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pillar_counts = articles['pillarName'].value_counts()\n",
    "new_pillar_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=articles, y='pillarName', kind='count', order=new_pillar_counts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "Sections are the next form of categorisation. Sections give us a better sense of the overall topic of the stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_counts = articles['sectionName'].value_counts()\n",
    "section_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.catplot(data=articles, y='sectionName', kind='count', aspect=1.5, order=section_counts.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how many sections are involved we may decide to keep only those above a certain threshold of presence in our dataset. This could be a top 10 or 20, or you could base it on some sort of summary metric of the counts such as categories above the mean or median count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_counts.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_avg_sections = section_counts[section_counts > section_counts.median()].index\n",
    "above_avg_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just go with a top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sections = section_counts.index[:10]\n",
    "top_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles[articles['sectionName'].isin(top_sections)]\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Section Content\n",
    "We may find interesting sections in our dataset and wonder why they're there. We can iterate through the titles and URLs of the section we're interested in to get a better sense of why they've been included.\n",
    "\n",
    "Below is a simple section filter but you could make it more complicated, such as limiting to after a time period, or in combination with a pillar classification for example.\n",
    "\n",
    "N.B Below we use `.head` to limit the number of results for demonstration purposes, but during analysis there is no reason you cannot remove it and to view all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SECTION_OF_INTEREST = 'Australia news' # Just change this to switch sections\n",
    "\n",
    "\n",
    "selected_data = articles[articles['sectionName'] == SECTION_OF_INTEREST].head(5)\n",
    "for index, row in selected_data.iterrows():\n",
    "    print(row['webTitle'])\n",
    "    print(row['webUrl'])\n",
    "    print('****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Tags are the last categorisation and they give us even more nuance in exactly what each story is about. However they are a little trickier to deal with because each story can have more than one tag associated with it. This presents us more of a challenge but also an opportunity for analysis too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the first item in our `tags` column, we can see that the value is actually a quite complex object. A `list` and then each item in the `list` is a `dictionary`. A lot of information is provided but for our purposes we just want the tag string, which is held under the `webTitle` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['tags'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each story could have multiple tags we're going to create a version of the `articles` dataframe where each row represents a single tag, and other story information like title, wordcount etc are duplicated. Pandas will keep track of which rows all refer to the same story using the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_per_line = articles.explode('tags')\n",
    "tag_per_line.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the length of the original `articles` dataframe against the new `tag_per_line` we can see that we have many more rows, one row per tag used in a story. We can also see the index values for our new dataframe are duplicated. This is because what was a single row, row `0` for example, is now three rows because the story had three tags. What was row `1` is now four rows, because the story at row `1` had four tags. These index values help us keep track of what rows 'go together' to  make a single story, which we'll need later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tag_per_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our `tags` column is similar in structure to our `fields` column, which we unpacked earlier using `.json_normalize`. We can do the same again to generate a seperate dataframe of `tag_data`. We also use a new method `.set_index()` to replace the default index that gets generated when `.json_normalize()` makes the new dataframe, with the index from `tag_per_line` which is keeping track of which rows go with which story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data = pd.json_normalize(tag_per_line['tags'])\n",
    "tag_data = tag_data.set_index(tag_per_line.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have index values keeping track of which row goes with which story, we could use those values to refer back to our original `articles` dataframe when we need additional information. However to simplify later tasks like printing out titles and urls, lets just copy the columns from `tag_per_line` into this new `tag_data` dataframe. The index lookup approach would be more space efficient but this isn't am issue with data this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data['wordcount'] = tag_per_line['wordcount']\n",
    "tag_data['article_title'] = tag_per_line['webTitle']\n",
    "tag_data['article_url'] = tag_per_line['webUrl']\n",
    "tag_data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite a lot of columns in this dataset we've made, and probably only need a few. Let's just overwrite `tag_data` with a view of it that only includes the columns we need just to keep things simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tag_data = tag_data[['webTitle','article_title','article_url','wordcount']]\n",
    "tag_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the count frequency of the different tags to get an overall picture like we did with sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts = tag_data['webTitle'].value_counts().head(20)\n",
    "top_tags = tag_counts.index\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=tag_data, y='webTitle', kind='count', aspect=1.5, order=top_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles by Tag\n",
    "Like before with sections, we can examine what stories are associated with each tag. The column names will be different but the mechanics are the same.\n",
    "N.B Below we use `.head` to limit the number of results for demonstration purposes, but during analysis there is no reason you cannot remove it and to view all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TAG_OF_INTEREST = 'Elon Musk' # Just change this to switch tags\n",
    "\n",
    "\n",
    "selected_data = tag_data[tag_data['webTitle'] == TAG_OF_INTEREST].head()\n",
    "\n",
    "for index, row in selected_data.iterrows():\n",
    "    print(row['article_title'])\n",
    "    print(row['article_url'])\n",
    "    print('****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this data more later when examining wordcounts, and looking at tag correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Section\n",
    "We defined `top_sections` earlier when we checked which sections had the highest number of stories. Here we'll use `.groupby` to get per section and per tag wordcounts. Word count is a good proxy for how much time was dedicated to a particular topic.\n",
    "\n",
    "Total word count tells us the overall time dedicated to the topic related to each section or topic, whilst taking an average tells us how much space was given per story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_wordcounts = articles.groupby('sectionName').agg(\n",
    "    avg_wordcount=('wordcount','mean'),\n",
    "    total_wordcount=('wordcount','sum')\n",
    ").sort_values('total_wordcount', ascending=False).loc[top_sections]\n",
    "\n",
    "section_wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use box plots to see the distribution of these word counts. Remember we already filtered the `articles` data so it only included stories in top sections, however we include the filtering here to clarify that it is necessary before visualisation to reduce visual clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = articles[articles['sectionName'].isin(top_sections)]\n",
    "sns.catplot(data=to_plot, y='sectionName', x='wordcount', kind='box', aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Tag\n",
    "For a similar summary, but by tag, we do the same, but we use the `tag_data` dataframe, and the `webTitle` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data.groupby('webTitle').agg(\n",
    "    avg_wordcount=('wordcount','mean'),\n",
    "    total_wordcount=('wordcount','sum')\n",
    ").sort_values('total_wordcount', ascending=False).loc[top_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = tag_data[tag_data['webTitle'].isin(top_tags)]\n",
    "sns.catplot(data=to_plot, y='webTitle', x='wordcount', kind='box', aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Correlation\n",
    "One analysis technique that is available to us is to examine the correlation of tags. What tags tend to co-occur in single stories, could this give us a sense of the themes or intersection of different topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create a matrix of tag counts. In the first stage we use `.get_dummies` to reshape our column of tag names so that each possible tag is given its own column, and a value of 1 is entered if that tag is present in the row, otherwise 0. \n",
    "\n",
    "(This may be a little confusing now but we're heading somewhere!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_matrix = pd.get_dummies(tag_data['webTitle'])\n",
    "tag_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take our `tag_matrix`, use our list of `top_tags` to ensure only columns representing our selected top tags remain. We do this to aid visualisation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tag_matrix = tag_matrix[top_tags].copy()\n",
    "tag_matrix.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment our `tag_matrix` is one row per tag per story, meaning that for every row *only one* of those columns will have a number 1 in it to represent a tag is associated with that story. In order to understand if certain tags correlate, if they go together, we need to simplify so that one row represents a story, and each column shows either a 0 or a 1 depending on whether the tag is present in that story.\n",
    "\n",
    "As stories can only use each tag once if we took all the rows for one single story, and for each column added the row values together, the result would be one row where 1 indicates if the tag is there or not because if it's not, we'd simply be adding together 0 for each row, resulting in 0. Using `groupby` we can grab each set of rows representing a single story, `.sum()` together the values in each column and then get one row back which provides this representation.\n",
    "\n",
    "Let's demo this with a simplified example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_matrix = pd.read_csv('toy_matrix.csv')\n",
    "toy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `story` column just represents the id or title of the story, and then we have a column for each of three different tags. You'll see that each row only has one `1` in it because it is one row per tag. What we want is one row per story, so just two rows, one for story A, one for story B that puts the values spread across multiple rows into just one row.\n",
    "\n",
    "You could probably do this in your head because really all we're saying is, for each story's subset of rows, if there is a `1` anywhere in the column, then the value is `1`, otherwise it's `0`. As we know a single story can only use a tag once, we can simplify this slightly complicated logic as just \"grab all the rows for a story and for each column, add the values together\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_matrix.groupby('story').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with our actual `tag_matrix`. As we want to group on the index of the dataframe rather than a column we don't have a column name to pass `.groupby()` as usual. However we can tell it to group by \"level 0\". Pandas refers to indexes as levels and on a regular dataframe with just a single index, there is only one level, level 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_matrix = tag_matrix.groupby(level=0).sum()\n",
    "tag_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get our correlation scores using `.corr`. This reshapes the data into a square, where both the rows and the columns represent tags, and the values represent the correleation between the two tags.\n",
    "\n",
    "- 0 Represents no correlation\n",
    "- 1 Represents the highest positive correlation, i.e. every story with tag `a` also includes tag `b`. \n",
    "- A negative value indicates negative correlation, i.e. the presence of tag `a` means that the presence of tag `b` is less likely.\n",
    "\n",
    "The 'diagnonal' of the matrix will always equal 1 as the presence of tag `a` will always be correlated with the presence of tag `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = tag_matrix.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the correlations for a specific tag by accessing its column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['ChatGPT'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag Heatmap\n",
    "We can also visualise these correlations using a heatmap. Using the `coolwarm` color scheme means colours run from deep blue to deep red. We set the `center` of the scale to 0 so that above zero, positive correlation, is a shade of red whilst below zero, negative correlation, is a shade of blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "sns.heatmap(correlations, cmap='coolwarm', center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Identifying multi-tag titles\n",
    "What if you wanted to understand WHY two tags correlate. Perhaps ones that are unexpected. You will need to identify which stories have both tags using our `tag_matrix`, and then use the index values to look up the correct rows in the `articles`. We can then iterate over them and view title and url like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_1 = 'ChatGPT'\n",
    "TAG_2 = 'Consciousness'\n",
    "tag_filter = (tag_matrix[TAG_1] == 1) & (tag_matrix[TAG_2] == 1)\n",
    "\n",
    "selected_story_index = tag_matrix[tag_filter].index\n",
    "selected_story_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = articles.loc[selected_story_index].head()\n",
    "\n",
    "for index, row in selected_data.iterrows():\n",
    "    print(row['webTitle'])\n",
    "    print(row['webUrl'])\n",
    "    print('****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "There will be many other ways in which this kind of data can be explored, depending on the kind of question you might have. However the above techniques give us a good overview of the data including the time period covered, the top topics, the type of content that has been collected (news, sport, opinion etc.) and allows us to get a sense of some correlations of the topics.\n",
    "\n",
    "## Exercises\n",
    "Explore your own data set from the Guardian API. Use the techniques above to get a better sense of what you've collected. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
